% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section*{Введение}
\thispagestyle{withCompileDate}

Большинство людей в современном мире уже не представляет повседневную жизнь без больших языковых моделей (LLM). Они встроены в поисковые системы, офисные пакеты, средства разработки, клиентскую поддержку и образовательные платформы. Крупные компании используют LLM для улучшения пользовательского опыта, автоматизации рутины, повышения производительности и индивидуализации сервисов под конкретного клиента, что ускоряет внедрение интеллектуальных функций в бизнес-продукты.

Расширение сфер применения LLM неизбежно ведёт к расширению ответственности за их поведение, особенно в крупных экономических, политических и социально значимых проектах. Безопасность ответов модели, предсказуемость её поведения и устойчивость к злонамеренным воздействиям становятся критически важными требованиями. Ошибки, предвзятости или управляемые отклонения в поведении системы могут приводить к репутационным рискам, финансовым потерям и нарушению нормативных требований.

Одним из ключевых векторов атак на LLM являются \emph{промт-инъекции} - техники, с помощью которых злоумышленник подталкивает модель к нарушению исходных инструкций, политик или ожидаемых ограничений. В результате «дружелюбный помощник» может начать генерировать неуместные, грубые или вредоносные ответы, рекомендовать действия, противоречащие политике провайдера, либо выдавать информацию, распространение которой запрещено. Такие сценарии подрывают доверие к системам на базе LLM и повышают стоимость их сопровождения и контроля.

В данной работе мы систематически рассматриваем и исследуем разновидности промт-инъекций с точки зрения их вредоносности и возможных последствий для поведения модели. На основе этого исследования формируется и размечается корпус данных, предназначенный для разработки бенчмарка, который позволит комплексно тестировать различные модели и оценивать их устойчивость к инструкционным атакам.

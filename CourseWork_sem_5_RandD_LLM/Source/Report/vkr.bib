@online{OWASP_LLM01_2025,
  author   = {{OWASP Foundation}},
  title    = {LLM01:  Prompt Injection —  OWASP Gen AI Security Risk Catalog},
  year     = {2025},
  url      = {https://genai.owasp.org/llmrisk/llm01-prompt-injection/},
  language = {english},
  note     = {Accessed 2025-11-28}
}

@article{LiuJia2023_FormalizingPromptInjection,
  author       = {Liu, Yupei and Jia, Yuqi and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
  title        = {Formalizing and Benchmarking Prompt Injection Attacks and Defenses},
  journal      = {USENIX Security Symposium 2024},
  year         = {2023},
  eprint       = {2310.12815},
  archivePrefix= {arXiv},
  primaryClass = {cs.CR},
  url          = {https://arxiv.org/abs/2310.12815},
  language     = {english}
}

@article{Chao2024_JailbreakBench,
  author       = {Chao, Patrick and Debenedetti, Edoardo and Robey, Alexander and Andriushchenko, Maksym and Croce, Francesco and Sehwag, Vikash and Dobriban, Edgar and Flammarion, Nicolas and Pappas, George J. and Tramèr, Florian and Hassani, Hamed and Wong, Eric},
  title        = {JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models},
  journal      = {Proceedings of NeurIPS 2024 — Datasets and Benchmarks Track},
  year         = {2024},
  eprint       = {2404.01318},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2404.01318},
  language     = {english}
}

@article{Wang2025_IsYourPromptSafe,
  author       = {Wang, Jiawen and Gupta, Pritha and Habernal, Ivan and Hüllermeier, Eyke},
  title        = {Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs},
  journal      = {arXiv Preprint},
  year         = {2025},
  eprint       = {2505.14368},
  archivePrefix= {arXiv},
  url          = {https://arxiv.org/abs/2505.14368},
  language     = {english}
}

@article{Saiem2025_SequentialBreak,
  author       = {Saiem, Bijoy Ahmed and Shanto, MD Sadik Hossain and Rashid, Md Rafi Ur},
  title        = {SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains},
  journal      = {ACL-SRW 2025},
  year         = {2025},
  url          = {https://aclanthology.org/2025.acl-srw.37.pdf},
  language     = {english}
}

@article{Chen2025_StrucQueryDefense,
  author       = {Chen, Sizhe and Wagner, David and Piet, Julien and Sitawarin, Chawin},
  title        = {StruQ: Defending Against Prompt Injection with Structured Queries},
  journal      = {USENIX Security Symposium 2025},
  year         = {2025},
  url          = {https://www.usenix.org/system/files/usenixsecurity25-chen-sizhe.pdf},
  language     = {english}
}

@article{Mathew2024_PromptInjectionReview,
  author       = {Mathew, Eleena Sarah},
  title        = {Enhancing Security in Large Language Models: A Comprehensive Review of Prompt Injection Attacks and Defenses},
  journal      = {TechRxiv Preprint},
  year         = {2024},
  url          = {https://www.techrxiv.org/handle/2248/37201},
  language     = {english}
}

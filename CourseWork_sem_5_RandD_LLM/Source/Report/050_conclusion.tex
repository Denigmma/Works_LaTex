% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section{Заключение}

В данной работе была рассмотрена проблема устойчивости больших языковых моделей к промт-инъекциям в банковско-финансовом домене и сформирован подход к построению специализированного корпуса для последующего бенчмаркинга. На основе анализа существующих атак и сценариев применения LLM была разработана схема классификации промт-инъекций, включающая базовые типы и расширенные подвиды, а также предложена структура метаданных, связывающая тип атаки, её цель и контекст применения.

В рамках решения задачи реализован программный комплекс, включающий два пайплайна генерации данных: базовый (\texttt{base}) и агентный (\texttt{agent}). Базовый пайплайн моделирует взаимодействие пользователя с ассистентом, работающим в финансово-ориентированном диалоге, тогда как агентный пайплайн фокусируется на синтетических системных промптах и ролях, приближенных к внутренним сервисам и инструментальным агентам. Для обоих направлений были выделены тематические области (банковские продукты, инвестиции, платежи, налоги, личные финансы), соответствующие подтемы, цели инъекций (ненормативная лексика, дискриминация, вредный код, незаконная деятельность, дезинформация и др.) и подтипы атак. Генерация организована батчево, стратификации по метаданным и централизованной фиксации результатов в базе данных.

Отдельный этап решения составляет подсистема валидации корпуса, основанная на подходе LLM-as-a-Judge. Для стратифицированной подвыборки примеров были определены критерии качества (тематическая релевантность, корректность реализации инъекции, структурированность, соответствие формату, согласованность пары \texttt{system\_text}/\texttt{user\_text} для агентного сценария) и интегральный показатель \texttt{overall} с бинарным флагом \texttt{pass\_flag}. Реализована устойчивая обработка ответов модели-судьи и сохранение результатов валидации в отдельную базу с последующим экспортом агрегированной статистики.

Проведённый эксперимент показал, что сгенерированный корпус обладает приемлемым уровнем качества: для базового пайплайна доля примеров, удовлетворяющих заданным требованиям, составляет порядка 75--80\,\%, тогда как агентный пайплайн демонстрирует существенно более высокую долю успешно прошедших валидацию примеров (около 95\,\%). При этом агентные инъекции, как правило, характеризуются более высокой оценкой технической корректности реализации, а проблемными зонами для базового пайплайна остаются сложные цели (самоповреждение, эксплицитный контент, насилие) и комбинированные подтипы атак (встроенные полезные нагрузки, дробление нагрузки, гипотетические сценарии и аргументация).

Разработанный корпус промт-инъекций и сопутствующая инфраструктура генерации и валидации создают основу для дальнейшего построения бенчмарка устойчивости LLM к инструкционным атакам. В дальнейшем планируется расширить набор моделей, протестированных на полученном датасете, провести масштабное сравнение популярных и перспективных языковых моделей, а также исследовать методы защиты и фильтрации с точки зрения баланса между безопасностью и полезностью. Отдельным направлением развития является перенос предложенной методологии на другие предметные области и языки, а также углублённый анализ ошибок и уязвимых сценариев.

Исходный код  доступен в публичном репозитории:
Исходный код доступен в публичном репозитории:
\href{https://github.com/Denigmma/Benchmark\_prompt\_injection}%
     {https://github.com/Denigmma/Benchmark\_prompt\_injection}.


Размеченный корпус промт-инъекций опубликован на платформе Kaggle и может использоваться для дальнейших исследований и разработки защитных механизмов:
Размеченный корпус доступен по адресу:
\href{https://www.kaggle.com/datasets/denismuradyan/dataset-prompt-injections-finance-sber-md}%
     {https://www.kaggle.com/datasets/denismuradyan/dataset-prompt-injections-finance-sber-md}.

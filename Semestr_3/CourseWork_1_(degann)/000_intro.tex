% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section*{Введение}
\thispagestyle{withCompileDate}

Дифференциальные уравнения  являются важным инструментом для описания процессов во многих областях науки и техники, включая физику, химию, биологию и экономику. Однако с усложнением моделей и увеличением размерности задачи традиционные численные методы решения дифференциальных уравнений начинают сталкиваться с проблемами, связанными с длительным временем вычислений и недостаточной точностью. Это стимулирует поиск новых подходов к решению таких задач, одним из которых является использование нейронных сетей для аппроксимации решений дифференциальных уравнений.

Пакет нейросетевой аппроксимации дифференциальных уравнений DEGANN предлагает инструменты для автоматического подбора оптимальной топологии нейронной сети для аппроксимации решений дифференциальных уравнений. Однако на текущий момент в пакете реализована только одна архитектура нейронной сети — полносвязная сеть, что ограничивает возможности системы и ухудшает точность аппроксимации сложных уравнений. Это создаёт необходимость в расширении пакета за счет добавления новых архитектур нейронных сетей, которые позволят улучшить точность и эффективность работы системы.

Рекуррентные нейронные сети (RNN — Recurrent Neural Network) представляют собой перспективный подход для задач аппроксимации дифференциальных уравнений, поскольку, в отличие от полносвязных сетей, они используют механизм запоминания состояний предыдущих слоёв. Это позволяет учитывать накопленную информацию из предыдущих шагов, что делает их более эффективными для моделирования сложных зависимостей и улучшает согласованность выходных значений. Однако RNN имеют известную проблему затухания градиента, из-за чего обучение становится затруднительным на длинных последовательностях.

Архитектура GRU (Gated Recurrent Unit, «Рекуррентный блок с управляемыми элементами») является усовершенствованной версией RNN и частично решает эту проблему благодаря использованию специальных элементов управления потоком данных — “элемента обнавления” (update gate) и “элемента сброса” (reset gate). Эти элементы регулируют, какую часть информации из предыдущих состояний следует сохранить, а какую — игнорировать, что позволяет сети избегать накопления нерелевантной информации. За счёт этого GRU демонстрирует высокую стабильность в обучении, сохраняя способность учитывать долгосрочные зависимости. Такое сочетание компактной структуры (меньшее количество параметров по сравнению с LSTM) и способности эффективно моделировать зависимости делает GRU особенно подходящей для задач, связанных с аппроксимацией решений сложных дифференциальных уравнений.

Целью данной работы является реализация архитектуры GRU в пакете DEGANN и проведение экспериментов, демонстрирующих её эффективность в задачах аппроксимации решений дифференциальных уравнений. Это позволит не только расширить функционал пакета, но и повысить его практическую применимость для сложных задач в научных и инженерных приложениях.

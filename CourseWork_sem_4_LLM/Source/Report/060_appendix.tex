% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\appendix
\section{Приложение}

\subsection{Примеры промптов и логика их формирования}

В этом приложении приведены реальные примеры промптов, используемых для режимов \emph{Structuring} и \emph{Codegen}, а также описание логики предварительной обработки HTML перед передачей в LLM.

\subsection{Пример промта для режима \emph{Structuring}}

Ниже приведены системная и пользовательская части промта, который отправляется LLM для прямого извлечения структурированных данных из текста страницы.

\begin{verbatim}
SYSTEM_PROMPT_STRUCTURING = """
Ты — языковая модель, выполняющая *исключительно* структурирование и очистку текста,
без добавления новых фраз, искажения смысла или выдумывания информации.
Твоя задача:

1. Убрать HTML-артефакты, дубли, повторяющиеся блоки, рекламные вставки и прочий шум,
   оставив только чистый, читаемый текст, максимально близкий к оригиналу.
2. Извлечь из очищенного текста именно ту информацию, которую запросил пользователь.
3. Сохранить также метаданные, переданные пользователем.

Ответ должен быть строго в формате JSON со следующими ключами:
{
  "query_data": "<информация, извлечённая по запросу пользователя>",
  "meta_data":  "<метаданные, указанные пользователем>"
}
"""

USER_PROMPT_STRUCTURING_TEMPLATE = """
Ниже приведён текст, извлечённый с веб-страницы:
{html}

1) Очисти и структурируй текст:
   - Убери HTML-теги, рекламные блоки и иной мусор.
   - Сделай текст читабельным.

2) Извлеки **только** ту информацию, которую я запрашиваю: {user_query}

3) Извлеки из текста следующие метаданные: {meta}

Верни результат строго в формате JSON с такими ключами:
{{
  "query_data": <информация по запросу пользователя>,
  "meta_data": <структура с запрошенными метаданными>
}}
"""
\end{verbatim}

\paragraph{Объяснение логики.}
\begin{itemize}
    \item \textbf{Системный промт} нацелен на жёсткое ограничение модели: запрещает «дописывать» или «домысливать» информацию, задаёт чёткую структуру ответа (два поля в JSON).
    \item \textbf{Пользовательский промт} включает:
    \begin{itemize}
        \item сам очищенный текст страницы (\verb|{html}|),
        \item описание нужной информации (\verb|{user_query}|),
        \item список метаданных (\verb|{meta}|), которые необходимо вернуть.
    \end{itemize}
    \item Модель возвращает JSON, где поле \texttt{query\_data} содержит весь текст, отфильтрованный по запросу, а \texttt{meta\_data} — переданные метаданные (например, URL, дату запроса, источник).
\end{itemize}

\subsection{Пример промта для режима \emph{Codegen}}

Ниже приведены системная и пользовательская части промта, который отправляется LLM для генерации Python-скрипта-парсера.

\begin{verbatim}
SYSTEM_PROMPT_CODEGEN = """
Ты — опытный Python-разработчик и специалист по надёжному парсингу HTML.
Твоя задача — генерировать **устойчивые**, не падающие скрипты, которые:
  - читают HTML из `stdin`;
  - парсят его с помощью `BeautifulSoup` из `bs4` и стандартных библиотек;
  - извлекают и печатают **всю** информацию, которую человек может увидеть на странице.

Обязательно:
  - **Проверяй** результат `soup.find(...)` на `None` перед тем, как брать `.text` или `.get(...)`:
      ```python
      block = soup.find('div', class_='foo')
      if block:
          print(block.text.strip())
      ```
  - Для списков элементов используй `for el in soup.find_all(...):`.
  - Для получения атрибутов всегда `el.get('href', '')`, а не `el['href']`.
  - Не придумывай селекторы — используй **только** реально существующие теги, классы и атрибуты в переданном HTML.
  - Код не должен генерировать необработанные исключения при отсутствии ожидаемых элементов.
  - Печатай результат через `print(...)` в понятном человеку виде.

Запрещено:
  - фразы «например», «может быть», «если», «предположим»;
  - выдумывать CSS-классы или атрибуты, которых нет в HTML.
"""

# User prompt template for codegen
USER_PROMPT_CODEGEN_TEMPLATE = """
Напиши **устойчивый** рабочий скрипт на Python, который:

  1) читает весь HTML через `sys.stdin`;
  2) парсит его через `bs4` и стандартные библиотеки;
  3) **извлекает и печатает только ту информацию, которую я запрашиваю**: {query};
  4) при отсутствии ожидаемых элементов корректно обрабатывает `None`/пустые списки;
  5) для доступа к атрибутам (`href`, `src` и т.д.) используй `.get('...', '')`;
  6) не допускай необработанных исключений.

Вот подсказка для точного и полноценного ответа
на запрос пользователя, где стоит искать информацию и как ее структурировать при выводе ответа: {hint}

Вот HTML, полученный с сайта:
```html
{html}
```
"""
\end{verbatim}

\paragraph{Объяснение логики.}
\begin{itemize}
\item \textbf{Системный промт} задаёт роль «опытного Python-разработчика» и строго определяет, какие приёмы кода разрешены (проверка на \texttt{None}, использование \verb|.get()| и \verb|for el in soup.find_all|). Это гарантирует, что сгенерированный скрипт будет устойчивым и не будет «падать» при отсутствии элементов.
\item \textbf{Пользовательский промт} включает:
\begin{itemize}
\item описание требуемой информации (\verb|{query}|),
\item подсказку о том, где искать данные и как структурировать вывод (\verb|{hint}|),
\item собственно «облегчённый» HTML (\verb|{html}|).
\end{itemize}
\item Модель генерирует функцию \texttt{parse(html)}, используя рекомендации из подсказки (\verb|{hint}|) и обеспечивая корректную обработку отсутствующих элементов.
\end{itemize}

\subsection{Логика подсказок (\emph{HintGen})}

Роль \emph{HintGen} — анализ HTML-кода и пользовательского запроса, после чего формулировка «чётких подсказок» для режима \emph{Codegen}. Пример системного и пользовательского промтов для \emph{HintGen}:

\begin{verbatim}
SYSTEM_PROMPT_HINTGEN="""
Ты — вспомогательная LLM-модель HintGen, которая анализирует HTML код и пользовательский запрос,
а затем формулирует чёткие подсказки для LLM-модели-парсера.

Твоя задача:
1. Определить, в каких блоках/тегах/классах или других местах страницы хранится информация,
которая соответствует запросу пользователя.
Далее тебе нужно сформулировать для LLM модели-парсера - где хранится информация для точного и полноценного ответа
на запрос пользователя и в как ее нужно структурировать при выводе ответа.
Ответь: перечисли селекторы (теги, классы, id) и опиши формат вывода - он должен быть читаемым для человека.
"""

USER_PROMPT_HINT_TEMPLATE="""
Сформулируй подсказки для LLM-модели-парсера.

Вот запрос пользователя: {query}

Вот HTML:
```html
{html}
```
"""

token_message_error="""
Информация слишком большая, остальная часть не поместилась. Работай с тем, что есть. НЕ ИСПОЛЬЗУЙ ИНФОРМАЦИЮ ЕСЛИ ОНА РЕЗКО ОБОРВАЛАСЬ И НЕЦЕЛОСТНАЯ
"""
\end{verbatim}

\paragraph{Объяснение логики.}
\begin{itemize}
\item \textbf{Задача \emph{HintGen}} — проанализировать HTML и пользовательский запрос (\verb|{query}|) и составить список селекторов (тегов, классов, атрибутов), где находится нужная информация, а также описать формат выводимых данных.
\item Результатом работы \emph{HintGen} является текст, вроде:
\begin{verbatim}
Для поиска цены используйте селектор div.price
Для названия товара — h1.title
Для описания — div.description > p
Формат вывода: print(f"Название: {title_text}")
print(f"Цена: {price_text}")
\end{verbatim}
\item Этот текст (\verb|{hint}|) затем вставляется в пользовательский промт для режима \emph{Codegen}, чтобы LLM-парсер знал, какие селекторы использовать.
\end{itemize}

\subsection{Описание обрезки «лишнего» HTML}

При передаче слишком большого HTML-кода в промт может быть превышён лимит токенов. Для этого используется следующая стратегия:
\begin{itemize}
\item Сначала рассчитывается количество токенов в пользовательском запросе (\verb|{user_query}|) и в сообщении об ошибке о слишком большом объёме (фрагмент \verb|token_message_error|), который добавляется в случае обрезки.
\item Если сумма токенов всего HTML-кода и токенов запроса превышает заранее установленное ограничение (\verb|MAX_INPUT_TOKENS|), то:
\begin{itemize}
\item Определяется, сколько токенов остаётся «доступным» для HTML после учёта запроса и сообщения об ошибке.
\item HTML-код конвертируется в токены и обрезается до этого числа доступных токенов.
\item Обрезанный HTML декодируется обратно и к нему приписывается сообщение об ошибке (\verb|token_message_error|), чтобы модель знала о влиянии обрезки.
\end{itemize}
\item Таким образом гарантируется, что итоговый промт не превысит лимит токенов, а модель получит максимально возможный объём релевантной информации.
\end{itemize}

В результате описанная схема применения \emph{HintGen}, \emph{Structuring} и \emph{Codegen} позволяет:
\begin{enumerate}
\item Автоматически определять, какую часть HTML передать в модель.
\item Составлять чёткие инструкции о том, где находится нужная информация и как её структурировать.
\item Генерировать безопасные и устойчиво работающие парсеры.
\end{enumerate}


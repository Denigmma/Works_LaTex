% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section*{Введение}
\thispagestyle{withCompileDate}

Веб-пространство сегодня характеризуется высокой технологической гетерогенностью:
существуют как простые статические HTML-страницы, так и сложные одностраничные приложения (SPA),
построенные с использованием React, Vue и других JavaScript-фреймворков.
Структура DOM, расположение элементов и способы загрузки данных могут существенно различаться от проекта к проекту.
В таких условиях создание универсального парсера, способного корректно извлекать данные с произвольного сайта,
оказывается невыполнимой задачей. Разработчик вынужден тратить значительное время
на анализ HTML-разметки, подбор селекторов и адаптацию к особенностям каждой конкретной страницы,
а при изменении верстки — постоянно обновлять написанный код.

Это заставляет задуматься об альтернативных способах парсинга.
В последние годы, большие языковые модели - (LLM), показали впечатляющие результаты в работе.
В контексте парсинга веб‐страниц - их использование позволяет реализовать автоматизированную систему парсинга.

В моем решении используется два подхода использования LLM:
\begin{itemize}
	\item для прямого структурирования содержимого: на вход модели подаётся очищенный HTML вместе с описанием запроса, и в ответ модель возвращает готовую структурированную
информацию, релевантную запросу пользователя.
	\item для генерации программного кода (Python‐скрипт),
который в последующем запускается для извлечения данных с конкретного сайта.
\end{itemize}

Однако, обращение к LLM для каждого нового запроса
требует значительных вычислительных ресурсов и занимает длительное время.
В этой работе предложена архитектура с кэшированием: при использовании второго подхода (генерации кода)
каждое сгенерированное решение кэшируется сохраняется в базу данных. При повторных запросах к тому же ресурсу запускается
уже готовый скрипт, что позволяет избежать дополнительных обращений к LLM и существенно ускоряет извлечение данных.

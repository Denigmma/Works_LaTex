% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section*{Введение}
\thispagestyle{withCompileDate}

Веб-пространство сегодня характеризуется высокой технологической гетерогенностью:
существуют как простые статические HTML-страницы \cite{RequestsDocumentation}, так и сложные одностраничные приложения (SPA),
построенные с использованием React, Vue и других JavaScript-фреймворков \cite{W3Techs2024}.
Структура DOM, расположение элементов и способы загрузки данных могут существенно различаться от проекта к проекту \cite{Cribbs2019}.
В таких условиях создание универсального парсера, способного корректно извлекать данные с произвольного сайта,
оказывается практически невыполнимой задачей. Разработчик вынужден тратить значительное время
на анализ HTML-разметки, подбор селекторов и адаптацию к особенностям каждой конкретной страницы,
а при изменении вёрстки — постоянно обновлять написанный код \cite{Richardson2013}.

Это стимулирует поиск альтернативных способов извлечения данных.
В последние годы большие языковые модели (LLM) продемонстрировали впечатляющие результаты в задачах анализа и генерации текстов \cite{Brown2020,Kalyan2023}.
В контексте парсинга веб-страниц их применение позволяет реализовать автоматизированную систему, способную адаптироваться к различным структурам HTML-страниц
и генерировать код для извлечения необходимых данных \cite{Kolluru2023HybridParsing}.

В данной работе используются два основных концептуальных подхода с LLM:
\begin{itemize}
	\item \emph{Structuring}: на вход модели подаётся очищенный HTML в сочетании с описанием требуемых полей, и в ответ модель возвращает готовую извлечённую информацию в формате JSON \cite{Brown2020}.
	\item \emph{Codegen}: модель генерирует программный код на Python, который затем выполняется локально для извлечения данных с конкретного сайта \cite{Dong2022CacheLLM}.
\end{itemize}

Однако обращение к LLM для каждого нового запроса требует значительных вычислительных ресурсов и занимает длительное время \cite{OpenAI2023Costs}.
В настоящей работе предложена архитектура с кэшированием: при использовании подхода Codegen каждое сгенерированное решение сохраняется в базу данных (SQLite и ChromaDB) \cite{Reimers2019}.
При повторных запросах к тому же ресурсу повторное обращение к LLM не выполняется, а используется уже готовый скрипт, что позволяет существенно ускорить процесс извлечения данных.

Введение. Дифференциальные уравнения (ДУ) играют ключевую роль в самых разных областях науки и техники. Решение таких уравнений – задача, требующая больших вычислений. С усложнением и увеличением размерности задачи эффективность традиционных численных методов решения дифференциальных уравнений снижается, что приводит к увеличению времени вычислений и потере точности [1]. В связи с указанными проблемами возникает необходимость в поиске альтернативных методов решения таких задач. Одним из них является использование нейронных сетей, которые могут аппроксимировать функции, описывающие ДУ. Такой подход используется в пакете нейросетевой аппроксимации ДУ DEGANN [2]. На данный момент функциональность пакета DEGANN ограничена одной архитектурой НС – многослойным перцептроном (MLP – Multi-Layer Perceptron). Эта архитектура может быть недостаточно эффективна в некоторых задачах аппроксимации ДУ, так как MLP не достаточно хорошо улавливает последовательные зависимости в данных [3]. В отличие от MLP рекуррентные нейронные сети (RNN – Recurrent Neural Network) [4] специально предназначены для работы с последовательностями, которые очень важны в анализе нелинейных ДУ. Однако RNN имеют известную проблему затухания и взрыва градиента, из-за чего обучение становится затруднительным на длинных последовательностях [5]. Архитектура (GRU – Gated Recurrent Unit) является усовершенствованной версией RNN и частично решает эту проблему благодаря использованию элементов управления потоком данных. Внедрение GRU в пакет DEGANN может помочь оптимизировать и улучшить качество аппроксимации ДУ. 2. Реализация и интеграция в пакет. Для реализации архитектуры Gated Recurrent Unit в пакете DEGANN разработан класс TensorflowGRUNet, который наследуется от tf.keras.Model. Первоначально создаётся список GRU-слоёв, с заданными функциями активации (основная – tanh, рекуррентная – sigmoid) и инициализаторами весов и смещений. Последовательная обработка входных данных этими слоями передаётся в полносвязный выходной слой с линейной активацией, что обеспечивает регрессионное предсказание. Логика компиляции модели выполняется с помощью реализованного метода custom_compile, который отвечает за настройку оптимизатора, функции потерь, метрик и т. д. Метод to_dict экспортирует конфигурацию модели в виде словаря для дальнейшей интеграции с другими компонентами DEGANN. В методе call реализован прямой проход через нейросеть, а вычисление градиентов производится стандартными средствами TensorFlow. Разработанная оболочка обеспечивает более гибкую настройку и унифицированный интерфейс для работы модели в системе. Основным элементом реализации является класс TensorflowGRUNet, который осуществляет следующие этапы: построение и инициализация GRU-слоёв, каждый из которых имеет число нейронов, определяемое параметром block_size; настройка ключевых параметров: input_size – размер входных данных (количество признаков); output_size – размер выхода; activation_func и recurrent_activation – функции активации для основного и рекуррентного состояний (по умолчанию tanh и sigmoid); weight и biases – инициализаторы весов и смещений; организация прямого прохода данных через сеть с помощью метода call, в этом методе входные данные последовательно обрабатываются всеми GRU-слоями, а затем результат передаётся в выходной полносвязный слой с линейной активацией, что оптимально для регрессионных задач. Интеграция архитектуры GRU в DEGANN осуществляется посредством параметра net_type, передаваемого при создании модели через базовый класс IModel. При установке net_type="GRUNet" внутри IModel вызывается метод, создающий экземпляр класса TensorflowGRUNet. Такой подход позволил сохранить единообразный интерфейс работы с моделями различных типов, не нарушая общую модульную структуру проекта, что обеспечивает возможность использования стандартных методов обучения, тестирования и экспорта моделей. 3. Валидация решения. В данном разделе представлено экспериментальное сравнение эффективности двух архитектур, реализованных в DEGANN: GRUNet (на основе рекуррентных слоёв GRU) и DenseNet (традиционный многослойный перцептрон, MLP). 3.1. Параметры эксперимента. Цель эксперимента – сравнить сходимость двух архитектур при одинаковых гиперпараметрах (3 слоя, 30 нейронов в каждом, функция потерь MSE, метрики MAPE, R2) и фиксированных временных промежутках обучения. Конфигурация тестирующей системы: CPU: Intel i5 1240p | GPU: Iris Xe Graphics G7 80EUs | RAM: 16 gb. 3.2. Выбор тестовых функций и их обоснование. Для проверки моделей были выбраны следующие функции, встроенные в DEGANN, которые представляют собой достаточно сложные для аппроксимации зависимости: функция демонстрирует периодическую нелинейность, что позволяет проверить способность модели захватывать колебательные процессы; функция обладает неоднородной зависимостью, где квадратичный рост сочетается с особенностями в малых значениях x, что требует от модели адаптивного поведения в различных областях определения; функция включает логарифмическое преобразование и экспоненциальные эффекты, ещё более усложняет задачу аппроксимации за счёт усиленной нелинейности. 3.3. Параметры моделей. Для каждой архитектуры эксперименты проводятся при фиксированном времени обучения, в частности результаты тестирования собираются для трёх вариантов времени обучения: 15, 30 и 45 секунд. При каждом варианте фиксируется функция потерь – Mean Squared Error (MSE), метрики – Mean Absolute Percentage Error (MAPE) и коэффициент детерминации (R2), а также затраты по памяти. Таким образом, основная задача эксперимента состоит в сравнении показателей качества аппроксимации для GRUNet и DenseNet при идентичных условиях обучения, при этом важно отметить, что использование одинаковых гиперпараметров позволяет провести объективное сравнение, где разница в результатах обусловлена внутренней структурой моделей, а не различиями в настройках. 3.4. Результаты эксперимента. В таблицах 1–3 приведены результаты эксперимента для каждого из выбранных интервалов времени обучения. 3.5. Анализ. Средние значения метрик MAPE, коэффициента детерминации (R2) и потребляемой памяти для каждой архитектуры и временного интервала следующие: время обучения 15 секунд: GRUNet: MAPE = 6,777, R2 = 0,908, память = 112,09 MB; DenseNet: MAPE = 15,514, R2 = 0,844, память = 36,92 MB; время обучения 30 секунд: GRUNet: MAPE = 3,895, R2 = 0,983, память = 114,76 MB; DenseNet: MAPE = 6,199, R2 = 0,969, память = 39,75 MB; время обучения 45 секунд: GRUNet: MAPE = 2,811, R2 = 0,991, память = 118,41 MB; DenseNet: MAPE = 5,406, R2 = 0,978, память = 44,89 MB. Анализ проведённых экспериментов демонстрирует, что архитектура GRUNet по метрикам MAPE и коэффициенту детерминации (R2) при идентичных гиперпараметрах и фиксированном времени обучения обеспечивает более высокое качество аппроксимации целевых функций по сравнению с DenseNet. 4. Заключение. Несмотря на несколько более высокие затраты памяти порядка 80 МБ не является значительной для пользователей в большинстве практических конфигураций, в отличие от времени. GRUNet демонстрирует более быструю сходимость, обеспечивая лучшее качество аппроксимации за меньшее время. Это делает данную архитектуру перспективной для задач аппроксимации дифференциальных уравнений в рамках пакета DEGANN, где важна эффективность обучения при сохранении высокой точности результатов.
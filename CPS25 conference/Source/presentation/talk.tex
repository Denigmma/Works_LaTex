% !TEX TS-program = xelatex
% !BIB program = bibtex
% !TeX spellcheck = ru_RU
% !TEX root = talk.tex

\documentclass
  [ russian
  , aspectratio=1610 % Для защит онлайн лучше использовать разрешение не 4х3
  ] {beamer}

\input{preamble.tex}
\makeatletter

\input{pretitle.tex}
\input{title.tex}

% Определяем команды для титульного слайда
\newcommand{\academicGroup}{\my@title@group@ru}
\newcommand{\advisorChair}{\my@title@chair@ru}

\title[GRU в DEGANN]{Реализация и внедрение архитектуры GRU в пакет нейросетевой аппроксимации ДУ DEGANN}

% Авторы: ФИО с указанием СПбГУ
\author[Мурадян Д. С., Алимов П. Г.]{Мурадян Денис Степанович, Алимов Павел Геннадьевич}

% Дополнительные команды для титульного слайда
\newcommand{\publicationRecommendation}{Рекомендация к публикации: доцент Гориховский В. И.}

\makeatother
\date{}
\begin{document}
{
\setbeamertemplate{footline}{}
% Титульный слайд
\begin{frame}
    \includegraphics[width=1.4cm]{figures/SPbGU_Logo.png}
    \vspace{-35pt}
    \hspace{-10pt}
    \begin{center}
        \begin{tabular}{c}
            \scriptsize{Санкт-Петербургский государственный университет} \\[2pt]
            \scriptsize{\advisorChair}
        \end{tabular}
        \titlepage
    \end{center}

    \btVFill

    {\scriptsize
        {\publicationRecommendation}
    }
    \makeatother
    \begin{center}
        \vspace{5pt}
        \scriptsize{Санкт-Петербург\\ 2025}
    \end{center}
\end{frame}
}

% Слайд 1: Введение
\begin{frame}{Введение}
    \begin{itemize}
        \item Дифференциальные уравнения (ДУ) играют важную роль в науке и технике, однако численные методы их решения могут быть ресурсоёмкими и давать неточные результаты.
        \item Использование нейронных сетей для аппроксимации функций, описывающих ДУ, предлагает альтернативный подход.
        \item Функциональность DEGANN ограничена применением многослойного перцептрона (MLP), который не достаточно хорошо улавливает последовательные зависимости в данных.
        \item Рекуррентные нейронные сети, специально предназначены для работы с последовательностями и могут повысить эффективность аппроксимации.
    \end{itemize}
\end{frame}

% Слайд 2: Реализация и интеграция в DEGANN
\begin{frame}{Реализация и интеграция в DEGANN}
    \begin{itemize}
        \item Разработан класс \texttt{TensorflowGRUNet} на базе \texttt{tf.keras.Model}.
        \item Основные этапы реализации:
              \begin{itemize}
                  \item Создание списка GRU-слоёв с активацией \texttt{tanh} для основного состояния и \texttt{sigmoid} для рекуррентного.
                  \item Последовательная обработка входных данных через GRU-слои с последующим выводом через полносвязный слой с линейной активацией для регрессионного предсказания.
              \end{itemize}
        \item В модели реализованы методы для настройки оптимизатора, функции потерь, метрик и экспорта конфигурации, а также прямой проход данных.
        \item Интеграция в DEGANN осуществляется через параметр \texttt{net\_type} с сохранением общего интерфейса.
    \end{itemize}
\end{frame}

% Слайд 3: Валидация решения: Экспериментальная установка
\begin{frame}{Валидация решения: Экспериментальная установка}
    \begin{itemize}
        \item \textbf{Цель эксперимента:} показать, что при идентичных гиперпараметрах GRUNet обеспечивает более точное приближение целевых значений функций за счёт учета последовательных зависимостей.
        \item Сравнивались две архитектуры:
              \begin{itemize}
                  \item GRUNet (на основе GRU)
                  \item DenseNet (традиционный MLP)
              \end{itemize}
        \item Фиксированные параметры эксперимента:
              \begin{itemize}
                  \item 3 слоя, 30 нейронов в каждом.
                  \item Функция потерь: MSE; метрики: MAPE, \(R^2\).
                  \item Время обучения: 15, 30 и 45 секунд.
              \end{itemize}
    \end{itemize}
\end{frame}

% Слайд 4: Выбор тестовых функций и их обоснование
\begin{frame}{Выбор тестовых функций и их обоснование}
    \begin{itemize}
        \item Для проверки моделей выбраны сложные для аппроксимации функции, каждая из которых предъявляет уникальные требования:
              \begin{itemize}
                  \item \(\displaystyle f_{\sin}(x)=\sin(10x)\) – демонстрирует периодическую нелинейность, позволяющую оценить способность модели захватывать колебательные процессы.
                  \item \(\displaystyle f_{\text{hyperbol}}(x)=\frac{x^2+0.5}{x+0.1}\) – обладает неоднородной зависимостью, сочетая квадратичный рост с особенностями при малых значениях \(x\).
                  \item \(\displaystyle f_{\text{hardsin}}(x)=\sin\Bigl(\ln\bigl(x^{\sin(10x)}\bigr)\Bigr)\) – включает логарифмическое преобразование и экспоненциальные эффекты, что усиливает нелинейность.
              \end{itemize}
        \item Такой набор функций позволяет всесторонне оценить эффективность сравниваемых архитектур.
    \end{itemize}
\end{frame}

% Слайд 5: Результаты эксперимента и анализ
\begin{frame}{Результаты эксперимента и анализ}
    \begin{itemize}
        \item Для каждой архитектуры были подсчитаны средние значения метрик для разных временных интервалов:
              \begin{itemize}
                  \item \textbf{15 секунд:}
                        \begin{itemize}
                            \item GRUNet: MAPE = 6.78, \(R^2\) = 0.908, Память = 112.09 MB.
                            \item DenseNet: MAPE = 15.51, \(R^2\) = 0.844, Память = 36.92 MB.
                        \end{itemize}
                  \item \textbf{30 секунд:}
                        \begin{itemize}
                            \item GRUNet: MAPE = 3.90, \(R^2\) = 0.983, Память = 114.76 MB.
                            \item DenseNet: MAPE = 6.20, \(R^2\) = 0.969, Память = 39.75 MB.
                        \end{itemize}
                  \item \textbf{45 секунд:}
                        \begin{itemize}
                            \item GRUNet: MAPE = 2.81, \(R^2\) = 0.991, Память = 118.41 MB.
                            \item DenseNet: MAPE = 5.41, \(R^2\) = 0.978, Память = 44.89 MB.
                        \end{itemize}
              \end{itemize}
        \item Анализ показывает, что GRUNet обеспечивает более высокое качество аппроксимации при идентичных условиях обучения, несмотря на более высокие затраты памяти.
    \end{itemize}
\end{frame}

% Слайд 6: Заключение
\begin{frame}{Заключение}
    \begin{itemize}
        \item Внедрение архитектуры GRU в DEGANN значительно повышает точность аппроксимации ДУ.
        \item Преимущество GRUNet обусловлено её способностью учитывать последовательные зависимости, что приводит к более быстрой сходимости.
        \item Несколько более высокие затраты памяти не критичны для большинства практических задач.
    \end{itemize}
\end{frame}

\end{document}

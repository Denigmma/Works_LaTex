\documentclass[a4paper]{article}
\usepackage{pmstyle}
\usepackage{cite}
\usepackage{hyperref}

\begin{document}
\hyphenation{СПбГУ}


% УДК для статьи берется со страницы http://teacode.com/online/udc/
\udk{УДК 004.89}

\author{Мурадян~Д.\:С., Алимов~П.\:Г.}

%%%%%%%%%   Название статьи
\title{Реализация и внедрение архитектуры\\ Gated Recurrent Unit\\ в пакет нейросетевой аппроксимации дифференциальных уравнений\\ DEGANN}

\renewcommand{\thefootnote}{ }
{\footnotetext{{\it Мурадян Денис Степанович} -- студент, Санкт-Петербургский государственный университет, e-mail: muradyan.denis@inbox.ru, тел.: +79052045723;}}
{\footnotetext{{\it Алимов Павел Геннадьевич} -- магистрант, Санкт-Петербургский государственный университет, e-mail: st076209@student.spbu.ru, тел.: +79045181399;}}
%{\footnotetext{{\it Гориховский Вячеслав Игоревич } -- доцент, Санкт-Петербургский государственный университет; e-mail: gorihovskyvyacheslav@gmail.com}}
% если по ГРАНТУ
{\footnotetext{Работа выполнена при финансовой поддержке поддержке СПБГУ, грант № 116636233}} % и/или СПбГУ, НИР № 0.00.000.0000}}
%(Pure ID 116636233).

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%                РЕКОМЕНДАЦИЯ К ПУБЛИКАЦИИ                %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Рекомендация научного руководителя (необходимо для студентов и аспирантов).

%%% Рекомендовано доцентом ...
\recdotz{Гориховским~В.\:И.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\razdel{Введение}
Дифференциальные уравнения (ДУ) играют ключевую роль в самых разных областях науки и техники.
Решение таких уравнений -- задача, требующая больших вычислений.
С усложнением и увеличением размерности задачи эффективность традиционных
численных методов решения дифференциальных уравнений снижается, что приводит
к увеличению времени вычислений и потере точности\cite{SODE}.

В связи с указанными проблемами, возникает необходимость в поиске альтернативных методов решения таких задач.
Одним из них является использование нейронных сетей,
которые могут аппроксимировать функции, описывающие ДУ.
Такой подход используется в пакете нейросетевой аппроксимации ДУ DEGANN\cite{degann}.

На данный момент функциональность пакета DEGANN ограничена одной архитектурой НС – многослойным перцептроном
(MLP -- Multi-Layer Perceptron).
Эта архитектура может быть недостаточно эффективна в некоторых задачах аппроксимации
ДУ, так как MLP не достаточно хорошо улавливает последовательные зависимости в данных\cite{odenn}. В отличие от MLP,
рекуррентные нейронные сети (RNN -- Recurrent Neural Network)\cite{IBM_RNN} специально предназначены для работы с
последовательностями, которые очень важны в анализе нелинейных ДУ.

Однако RNN имеют известную проблему затухания и взрыва градиента, из-за чего обучение становится затруднительным
на длинных последовательностях\cite{BG}.
Архитектура (GRU -- Gated Recurrent Unit) является усовершенствованной версией RNN и
частично решает эту проблему благодаря использованию элементов управления потоком данных.

Внедрение GRU в пакет DEGANN может помочь оптимизировать и улучшить качество
аппроксимации ДУ.



\razdel{Реализация и интеграция в пакет} Для реализации архитектуры \texttt{Gated Recurrent Unit} в пакете DEGANN
разработан класс \texttt{TensorflowGRUNet}, который наследуется от \texttt{tf.keras.Model}.


Первоначально создаётся список GRU-слоёв, с заданными функциями активации
(основная -- \textit{tanh}, рекуррентная -- \textit{sigmoid}) и инициализаторами весов и смещений.
Последовательная обработка входных данных этими слоями передаётся в полносвязный выходной слой с
линейной активацией, что обеспечивает регрессионное предсказание.


Логика компиляции модели выполняется с помощью реализованного метода \texttt{custom\_compile},
который отвечает за настройку оптимизатора, функции потерь, метрик и т.~д.
Метод \texttt{to\_dict}, экспортирует конфигурацию модели в виде словаря для дальнейшей интеграции
с другими компонентами DEGANN. В методе \texttt{call} реализован прямой проход через нейросеть,
а вычисление градиентов производится стандартными средствами TensorFlow.
Разработанная оболочка обеспечивает более гибкую настройку и унифицированный интерфейс для работы модели в системе.


Основным элементом реализации является класс\linebreak \texttt{TensorflowGRUNet}, который осуществляет следующее этапы:
\begin{itemize}
  \item построение и инициализация GRU-слоёв, каждый из которых имеет число нейронов, определяемое параметром \textbf{block\_size};
  \item настройка ключевых параметров:
    \begin{itemize}
      \item \textbf{input\_size} -- размер входных данных (количество признаков);
      \item \textbf{output\_size} -- размер выхода;
      \item \textbf{activation\_func} и \textbf{recurrent\_activation} -- функции активации для основного и рекуррентного состояний (по умолчанию \textit{tanh} и \textit{sigmoid});
      \item \textbf{weight} и \textbf{biases} -- инициализаторы весов и смещений;
    \end{itemize}
  \item организация прямого прохода данных через сеть с помощью метода \texttt{call}.
  В этом методе входные данные последовательно обрабатываются всеми GRU-слоями, а затем результат передается в выходной полносвязный слой с линейной активацией, что оптимально для регрессионных задач.
\end{itemize}

Интеграция архитектуры GRU в DEGANN осуществляется посредством параметра \texttt{net\_type},
передаваемого при создании модели через базовый класс \texttt{IModel}.
При установке \texttt{net\_type="GRUNet"} внутри \texttt{IModel} вызывается метод,
создающий экземпляр класса \texttt{TensorflowGRUNet}.
Такой подход позволил сохранить единообразный интерфейс работы с моделями различных типов (например, с DenseNet или другими),
не нарушая общую модульную структуру проекта.
Это обеспечивает возможность использования стандартных методов обучения, тестирования и экспорта моделей.


\razdel{Валидация решения} В данном разделе представлено экспериментальное сравнение эффективности двух архитектур, реализованных в DEGANN:
GRUNet (на основе рекуррентных слоёв GRU) и DenseNet (традиционный многослойный перцептрон, MLP).


\podrazdel{Параметры эксперимента} Цель эксперимента -- сравнить сходимость двух архитектур при одинаковых гиперпараметрах (3 слоя, 30 нейронов
в каждом, функция потерь MSE, метрики MAPE, $R^2$) и фиксированных временных промежутках обучения.

\textit{Конфигурация тестирующей системы:} CPU: Intel i5 1240p | GPU: Iris Xe Graphics G7 80EUs | RAM: 16 gb

\podrazdel{Выбор тестовых функций и их обоснование} Для проверки моделей были выбраны следующие функции, встроенные в DEGANN,
которые представляют собой достаточно сложные для аппроксимации зависимости:

\begin{itemize}
    \item функция $\displaystyle f_{\sin}(x)=\sin(10x)$ демонстрирует периодическую нелинейность,
что позволяет проверить способность модели захватывать колебательные процессы;
\item функция $\displaystyle f_{\text{hyperbol}}(x)=\frac{x^2+0,\!5}{x+0,\!1}$ обладает неоднородной
зависимостью, где квадратичный рост сочетается с особенностями в малых значениях $x$, что требует
от модели адаптивного поведения в различных областях определения;
\item функция $\displaystyle f_{\text{hardsin}}(x)=\sin\Bigl(\ln\bigl(x^{\sin(10x)}\bigr)\Bigr)$,
включающая логарифмическое преобразование и экспоненциальные эффекты, ещё более усложняет задачу
аппроксимации за счёт усиленной нелинейности.
\end{itemize}


Такой набор тестовых функций позволяет всесторонне оценить эффективность сравниваемых
архитектур, поскольку каждая из них предъявляет уникальные требования к способности
модели моделировать сложные, неоднородные зависимости.


\podrazdel{\textbf{Параметры моделей}} Для каждой архитектуры эксперименты проводятся при фиксированном времени обучения.
В частности, результаты тестирования собираются для трёх вариантов времени обучения: 15, 30 и 45 секунд.
При каждом варианте фиксируется функция потерь -- Mean Squared Error (MSE), метрики -- Mean Absolute Percentage Error (MAPE)
и коэффициент детерминации ($R^2$),а также затраты по памяти.
%\begin{itemize}
%    \item \textbf{Функция потерь:} Mean Squared Error (MSE),
%    \item \textbf{Метрики:} Mean Absolute Percentage Error (MAPE) и коэффициент детерминации ($R^2$)
%    \item Затраты памяти
%\end{itemize}

Таким образом, основная задача эксперимента состоит в сравнении показателей качества
аппроксимации для GRUNet и DenseNet при идентичных условиях обучения.
При этом важно отметить, что использование одинаковых гиперпараметров позволяет провести объективное сравнение,
где разница в результатах обусловлена внутренней структурой моделей, а не различиями в настройках.


\podrazdel{Результаты эксперимента} В таблицах 1--3 приведены результаты эксперимента для каждого из выбранных интервалов времени обучения.
\\

\Table{Результаты эксперимента (время обучения 15 секунд)}{|p{2cm}|p{1.5cm}|p{1.1cm}|p{0.7cm}|p{1cm}|p{0.7cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet & sin & 111,36 & 0,003 & 4,149 & 0,994 \nextline
GRUNet & hardsin & 112,45 & 0,045 & 14,647 & 0,738 \nextline
GRUNet & hyperbol & 112,46 & 0,002 & 1,536 & 0,993 \nextline
DenseNet & sin & 35,00  & 0,091 & 23,095 & 0,812 \nextline
DenseNet & hardsin & 41,77  & 0,017 & 10,100 & 0,902 \nextline
DenseNet & hyperbol & 33,99 & 0,051 & 13,346 & 0,819 \nextline
}

\Table{Результаты эксперимента (время обучения 30 секунд)}{|p{2cm}|p{1.5cm}|p{1.1cm}|p{0.7cm}|p{1cm}|p{0.7cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet   & sin      & 115,39 & 0,003 & 4,169 & 0,994 \nextline
GRUNet   & hardsin  & 112,62 & 0,004 & 4,008 & 0,976 \nextline
GRUNet   & hyperbol & 116,27 & 0,005 & 3,508 & 0,980 \nextline
DenseNet & sin      & 37,52  & 0,012 & 7,509 & 0,975 \nextline
DenseNet & hardsin  & 44,78  & 0,005 & 5,101 & 0,971 \nextline
DenseNet & hyperbol & 36,95  & 0,011 & 5,989 & 0,961 \nextline
}

\Table{Результаты эксперимента (время обучения 45 секунд)}{|p{2cm}|p{1.5cm}|p{1.1cm}|p{0.7cm}|p{1cm}|p{0.7cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet   & sin       & 120,59 & 0,003 & 4,101 & 0,995 \nextline
GRUNet   & hardsin   & 116,84 & 0,002 & 3,158 & 0,987 \nextline
GRUNet   & hyperbol  & 117,79 & 0,002 & 1,174 & 0,992 \nextline
DenseNet & sin       & 38,98  & 0,007 & 7,339 & 0,985 \nextline
DenseNet & hardsin   & 47,71  & 0,006 & 5,741 & 0,962 \nextline
DenseNet & hyperbol  & 47,98  & 0,004 & 3,138 & 0,987 \nextline
}




\podrazdel{Анализ} Средние значения метрик MAPE, коэффициента детерминации ($R^2$) и потребляемой памяти для
каждой архитектуры и временного интервала следующие:
\begin{itemize}
    \item \textit{время обучения 15 секунд:}
        \\GRUNet: MAPE = 6,777, $R^2$ = 0,908, память = 112,09 MB;
        \\DenseNet: MAPE = 15,514, $R^2$ = 0,844, память = 36,92 MB;
    \item \textit{время обучения 30 секунд:}
        \\GRUNet: MAPE = 3,895, $R^2$ = 0,983, память = 114,76 MB;
        \\DenseNet: MAPE = 6,199, $R^2$ = 0,969, память = 39,75 MB;
    \item \textit{время обучения 45 секунд:}
        \\GRUNet: MAPE = 2,811, $R^2$ = 0,991, память = 118,41 MB;
        \\DenseNet: MAPE = 5,406, $R^2$ = 0,978, память = 44,89 MB.
\end{itemize}


Анализ проведённых экспериментов демонстрирует, что архитектура GRUNet (по метрикам MAPE и коэффициенту детерминации ($R^2$))
при идентичных гиперпараметрах и фиксированном времени обучения обеспечивает более высокое качество аппроксимации целевых функций
по сравнению с DenseNet.

\razdel{Заключение} Несмотря на несколько более высокие затраты памяти, разница в потреблении ресурсов
(порядка 80 МБ) не является значительной для пользователей в большинстве практических конфигураций,
в отличие от времени. GRUNet демонстрирует более быструю сходимость, обеспечивая лучшее качество
аппроксимации за меньшее время. Это делает данную архитектуру перспективной для задач
аппроксимации дифференциальных уравнений в рамках пакета DEGANN, где важна эффективность обучения при
сохранении высокой точности результатов.


\begin{thebibliography}{40}

\bibitem{SODE} Hairer E., Wanner G. Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems. – Heidelberg: Springer-Verlag, 1996. – 629 с.

\bibitem{degann} Пакет нейросетевой аппроксимации дифференциальных уравнений // DEGANN [Электронный ресурс]. – URL \url{https://pypi.org/project/degann/} (дата обращения: 10.03.2025).

\bibitem{odenn} Chen R.T.Q., Rubanova Y., Bettencourt J., Duvenaud D. Neural Ordinary Differential Equations // Advances in Neural Information Processing Systems. – 2018. – No 31. – P. 6571–6583.

\bibitem{IBM_RNN} Stryker C. What is a Recurrent Neural Network? // IBM [Электронный ресурс]. – 2024. – URL \url{https://www.ibm.com/think/topics/recurrent-neural-networks} (дата обращения: [укажите дату обращения]).

\bibitem{BG} Mosquera C., Van Houdt G., Napoles G. A review on the long short-term memory model // Artificial Intelligence Review. – 2020. – No 53. – P. 5929–5955.

\end{thebibliography}


\end{document}

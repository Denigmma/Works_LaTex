\documentclass[a4paper]{article}
\usepackage{pmstyle}
\usepackage{cite}
\usepackage{hyperref}

\begin{document}
\hyphenation{СПбГУ}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                     УДК | АВТОР | НАЗВАНИЕ СТАТЬИ               %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% УДК для статьи берется со страницы http://teacode.com/online/udc/
\udk{УДК 004.89}

\author{Мурадян~Д.\:С.}

%%%%%%%%%   Название статьи
\title{Реализация и внедрение архитектуры\\ Gated Recurrent Unit\\ в пакет нейросетевой аппроксимации дифференциальных уравнений\\ DEGANN}

\renewcommand{\thefootnote}{ }
{\footnotetext{{\it Гориховский Вячеслав Игоревич } -- доцент, Санкт-Петербургский государственный университет; e-mail: gorihovskyvyacheslav@gmail.com
}}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%                РЕКОМЕНДАЦИЯ К ПУБЛИКАЦИИ                %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Рекомендация научного руководителя (необходимо для студентов и аспирантов).

%%%% БЕЗ рекомендации
%\norec{}

%%%%% Рекомендовано профессором ...
%\recprof{Утешевым~А.\:Ю.}

%%% Рекомендовано доцентом ...
\recdotz{Гориховским~В.\:И.}

%%%% Рекомендовано старшим преподавателем ...
%\recsp{Ереминым~А.\:С.}

%%%% Рекомендовано ассистентом ...
%\recassist{Ереминым~А.\:С.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\razdel{Введение}
Дифференциальные уравнения играют ключевую роль в самых разных областях науки и техники.
Решение таких уравнений -- задача, довольно трудная, требующая больших вычислений.
С усложнением и увеличением размерности задачи -- традиционные численные методы решения
дифференциальных уравнений начинают сталкиваться с проблемами, связанными с длительным временем вычислений,
недостаточной точностью и так далее.

Это заставляет задуматься об альтернативных способах решения таких задач.
Одним из таких способов является использование нейронных сетей,
которые могут аппроксимировать сложные функции, описывающие дифференциальные уравнения.
Такой подход используется в пакете нейросетевой аппроксимации дифференциальных уравнений DEGANN.

Функционал этого пакета ограничен одним типом нейронной сети – многослойным перцептроном
(MLP -- Multi-Layer Perceptron).
Этот тип может быть недостаточно эффективен в некоторых задачах аппроксимации
дифференциальных уравнений, так как MLP не способен выявлять последовательные зависимости в данных, в отличие
от рекуррентных нейронных сетей (RNN -- Recurrent Neural Network), специально предназначенных для работы с
последовательностями, которые очень важны в анализе нелинейных дифференциальных уравнений.

Однако RNN имеют известную проблему затухания и взрыва градиента, из-за чего обучение становится затруднительным
на длинных последовательностях.
Архитектура GRU -- (Gated Recurrent Unit) является усовершенствованной версией RNN и
частично решает эту проблему благодаря использованию элементов управления потоком данных.

Внедрение этой архитектуры в пакет DEGANN должно помочь оптимизировать и улучшить качество
аппроксимации дифференциальных уравнений.



\razdel{Реализация и внедрение}

\podrazdel{Реализация}


Для реализации архитектуры GRU в пакете DEGANN использовались встроенные в
TensorFlow рекуррентные слои (\texttt{tf.keras.layers.GRU}).
Такой выбор позволил воспользоваться готовыми механизмами обновляющих и сбрасывающих гейтов,
а также стандартными функциями активации для работы со скрытыми состояниями,
что существенно упростило интеграцию новой топологии в общую архитектуру DEGANN.

Основным элементом данной реализации является класс \texttt{TensorflowGRUNet},
который наследуется от \texttt{tf.keras.Model}. Он отвечает за:
\begin{itemize}
  \item Построение и инициализацию GRU-слоёв, каждый из которых имеет число нейронов (определяемое параметром \textbf{block\_size}),
  \item Настройку ключевых параметров:
    \begin{itemize}
      \item \textbf{input\_size} --- размер входных данных (количество признаков);
      \item \textbf{output\_size} --- размер выхода;
      \item \textbf{activation\_func} и \textbf{recurrent\_activation} --- функции активации для основного и рекуррентного состояний (по умолчанию \textit{tanh} и \textit{sigmoid});
      \item \textbf{weight} и \textbf{biases} --- инициализаторы весов и смещений.
    \end{itemize}
  \item Организацию прямого прохода данных через сеть с помощью метода \texttt{call}.
  В этом методе входные данные последовательно обрабатываются всеми GRU-слоями, а затем результат передается в выходной полносвязный слой с линейной активацией, что оптимально для регрессионных задач.
\end{itemize}


\podrazdel{Внедрение}


Интеграция архитектуры GRU в DEGANN осуществляется посредством параметра \texttt{net\_type},
передаваемого при создании модели через базовый класс \texttt{IModel}.
При установке \texttt{net\_type="GRUNet"} внутри \texttt{IModel} вызывается метод,
создающий экземпляр класса \texttt{TensorflowGRUNet}.
Такой подход позволил сохранить единообразный интерфейс работы с моделями различных типов (например, с DenseNet или другими),
не нарушая общую модульную структуру проекта.

Особое внимание уделено совместимости новой реализации с существующей инфраструктурой DEGANN.
Это обеспечивает возможность использования стандартных методов обучения, тестирования и экспорта моделей.
Например, метод \texttt{ToDict} позволяет сохранить ключевые параметры модели (тип сети, количество слоёв,
число нейронов в каждом слое, размер выхода) в виде словаря для последующей интеграции с другими компонентами системы.


\razdel{Валидация}


\podrazdel{Параметры эксперемента}


В данном разделе представлено экспериментальное сравнение эффективности двух архитектур, реализованных в DEGANN: GRUNet (на основе рекуррентных слоёв GRU) и DenseNet (традиционный многослойный перцептрон, MLP). Цель эксперимента — продемонстрировать, что при одинаковых гиперпараметрах (5 слоёв, 30 нейронов в каждом, функция потерь MSE, метрики MAPE и $R^2$) и фиксированном времени обучения архитектура GRU обеспечивает более точное приближение целевых функций за счёт своей способности учитывать последовательные зависимости в данных.

\textbf{Тестовые функции.} Для проверки моделей используются следующие функции, встроенные в DEGANN:
\begin{itemize}
    \item $\displaystyle f_{\sin}(x)=\sin(10x)$,
    \item $\displaystyle f_{\text{hyperbol}}(x)=\frac{x^2+0.5}{x+0.1}$,
    \item $\displaystyle f_{\text{hardsin}}(x)=\sin\Bigl(\ln\bigl(x^{\sin(10x)}\bigr)\Bigr)$.
\end{itemize}


\textbf{Обоснование выбора тестовых функций.} Выбранные функции являются встроенными в DEGANN и представляют собой достаточно сложные для аппроксимации зависимости. Функция $\displaystyle f_{\sin}(x)=\sin(10x)$ демонстрирует периодическую нелинейность, что позволяет проверить способность модели захватывать колебательные процессы. Функция $\displaystyle f_{\text{hyperbol}}(x)=\frac{x^2+0.5}{x+0.1}$ обладает неоднородной зависимостью, где квадратичный рост сочетается с особенностями в малых значениях $x$, что требует от модели адаптивного поведения в различных областях определения. Функция $\displaystyle f_{\text{hardsin}}(x)=\sin\Bigl(\ln\bigl(x^{\sin(10x)}\bigr)\Bigr)$, включающая логарифмическое преобразование и экспоненциальные эффекты, ещё более усложняет задачу аппроксимации за счёт усиленной нелинейности. Такой набор тестовых функций позволяет всесторонне оценить эффективность сравниваемых архитектур, поскольку каждая из них предъявляет уникальные требования к способности модели моделировать сложные, неоднородные зависимости.

\textbf{Параметры:} Для каждой архитектуры эксперименты проводятся при фиксированном времени обучения. В частности, результаты тестирования собираются для трёх вариантов времени обучения: 15, 30 и 45 секунд. При каждом варианте фиксируются следующие показатели:
\begin{itemize}
    \item \textbf{Функция потерь:} Mean Squared Error (MSE),
    \item \textbf{Метрики:} Mean Absolute Percentage Error (MAPE) и коэффициент детерминации ($R^2$),
    \item Дополнительно, фиксируются затраты оперативной памяти и общее время обучения.
\end{itemize}

Таким образом, основная задача эксперимента состоит в сравнении показателей качества аппроксимации для GRUNet и DenseNet при идентичных условиях обучения. При этом важно отметить, что использование одинаковых гиперпараметров позволяет провести объективное сравнение, где разница в результатах обусловлена внутренней структурой моделей, а не различиями в настройках.


\podrazdel{Резултаты эксперемента}


Ниже приведены три таблицы, демонстрирующие структуру представления результатов эксперимента для каждого из выбранных интервалов времени обучения.


\Table{Результаты эксперимента (время обучения: 15 секунд)}{|p{2cm}|p{2cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Эпох} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet & hyperbol & 155 & 112.460 & 0.002 & 1.536 & 0.993 \nextline
GRUNet & sin      & 155 & 111.360 & 0.003 & 4.149 & 0.994 \nextline
GRUNet & hardsin  & 155 & 112.450 & 0.045 & 14.647 & 0.738 \nextline
DenseNet & sin    & 215 & 35.000  & 0.091 & 23.095 & 0.812 \nextline
DenseNet & hardsin& 215 & 41.770  & 0.017 & 10.100 & 0.902 \nextline
DenseNet & hyperbol & 215 & 33.990 & 0.051 & 13.346 & 0.819 \nextline
}


\Table{Результаты эксперимента (время обучения: 30 секунд)}{|p{2cm}|p{2cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Эпох} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet   & hardsin  & 330 & 112.62 & 0.004 & 4.008 & 0.976 \nextline
GRUNet   & hyperbol & 330 & 116.27 & 0.005 & 3.508 & 0.980 \nextline
DenseNet & sin      & 445 & 37.52  & 0.012 & 7.509 & 0.975 \nextline
DenseNet & hyperbol & 445 & 36.95  & 0.011 & 5.989 & 0.961 \nextline
DenseNet & hardsin  & 445 & 44.78  & 0.005 & 5.101 & 0.971 \nextline
GRUNet   & sin      & 345 & 115.39 & 0.003 & 4.169 & 0.994 \nextline
}


\Table{Результаты эксперимента (время обучения: 45 секунд)}{|p{2cm}|p{2cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Эпох} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet   & sin      & 450 & 120.59 & 0.003 & 4.101 & 0.995 \nextline
GRUNet   & hardsin  & 470 & 116.84 & 0.002 & 3.158 & 0.987 \nextline
GRUNet   & hyperbol & 450 & 117.79 & 0.002 & 1.174 & 0.992 \nextline
DenseNet & sin      & 530 & 38.98  & 0.007 & 7.339 & 0.985 \nextline
DenseNet & hyperbol & 600 & 47.98  & 0.004 & 3.138 & 0.987 \nextline
DenseNet & hardsin  & 600 & 47.71  & 0.006 & 5.741 & 0.962 \nextline
}


\podrazdel{Вывод}


Анализ проведённых экспериментов демонстрирует, что архитектура GRUNet при идентичных гиперпараметрах
и фиксированном времени обучения обеспечивает более высокое качество аппроксимации целевых функций
по сравнению с DenseNet.
По метрикам MAPE и коэффициенту детерминации ($R^2$)
наблюдается стабильное преимущество GRUNet, что указывает на его лучшую способность учитывать
последовательные зависимости в данных.
Несмотря на несколько более высокие затраты памяти, улучшенная точность модели оправдывает эти затраты,
что делает её перспективной для задач аппроксимации дифференциальных уравнений в рамках пакета DEGANN.

\end{document}

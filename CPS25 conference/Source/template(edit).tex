\documentclass[a4paper]{article}
\usepackage{pmstyle}
\usepackage{cite}
\usepackage{hyperref}

\begin{document}
\hyphenation{СПбГУ}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                     УДК | АВТОР | НАЗВАНИЕ СТАТЬИ               %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% УДК для статьи берется со страницы http://teacode.com/online/udc/
\udk{УДК 004.89}

\author{Мурадян~Д.\:С.}

%%%%%%%%%   Название статьи
\title{Реализация и внедрение архитектуры\\ Gated Recurrent Unit\\ в пакет нейросетевой аппроксимации дифференциальных уравнений\\ DEGANN}

\renewcommand{\thefootnote}{ }
{\footnotetext{{\it Мурадян Денис Степанович} -- студент бакалавриата, Санкт-Петербургский государственный университет; e-mail: muradyan.denis@inbox.ru}}
{\footnotetext{{\it Гориховский Вячеслав Игоревич } -- доцент, Санкт-Петербургский государственный университет; e-mail: gorihovskyvyacheslav@gmail.com}}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%                РЕКОМЕНДАЦИЯ К ПУБЛИКАЦИИ                %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Рекомендация научного руководителя (необходимо для студентов и аспирантов).

%%% Рекомендовано доцентом ...
\recdotz{Гориховским~В.\:И.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\razdel{Введение}
Дифференциальные уравнения играют ключевую роль в самых разных областях науки и техники.
Решение таких уравнений -- задача, довольно трудная, требующая больших вычислений.
С усложнением и увеличением размерности задачи -- традиционные численные методы решения
дифференциальных уравнений начинают сталкиваться с проблемами, связанными с длительным временем вычислений,
недостаточной точностью и так далее.

Это заставляет задуматься об альтернативных способах решения таких задач.
Одним из таких способов является использование нейронных сетей,
которые могут аппроксимировать сложные функции, описывающие дифференциальные уравнения.
Такой подход используется в пакете нейросетевой аппроксимации дифференциальных уравнений DEGANN.

Функционал этого пакета ограничен одним типом нейронной сети – многослойным перцептроном
(MLP -- Multi-Layer Perceptron).
Этот тип может быть недостаточно эффективен в некоторых задачах аппроксимации
дифференциальных уравнений, так как MLP не способен выявлять последовательные зависимости в данных, в отличие
от рекуррентных нейронных сетей (RNN -- Recurrent Neural Network), специально предназначенных для работы с
последовательностями, которые очень важны в анализе нелинейных дифференциальных уравнений.

Однако RNN имеют известную проблему затухания и взрыва градиента, из-за чего обучение становится затруднительным
на длинных последовательностях.
Архитектура GRU -- (Gated Recurrent Unit) является усовершенствованной версией RNN и
частично решает эту проблему благодаря использованию элементов управления потоком данных.

Внедрение этой архитектуры в пакет DEGANN должно помочь оптимизировать и улучшить качество
аппроксимации дифференциальных уравнений.



\razdel{Реализация и внедрение}


\podrazdel{Реализация}

Для реализации архитектуры GRU в пакете DEGANN разработан класс \texttt{TensorflowGRUNet},
наследуемый от \texttt{tf.keras.Model}. Сначала создаётся список GRU-слоёв с заданными функциями активации
(основная — \textit{tanh}, рекуррентная — \textit{sigmoid}) и инициализаторами весов и смещений.
Последовательная обработка входных данных этими слоями передаётся в полносвязный выходной слой с
линейной активацией, что обеспечивает регрессионное предсказание.

Логика компиляции модели выполняется с помощью реализованного метода \texttt{custom\_compile}
— он отвечает за настройку оптимизатора, функции потерь и метрик и так далее, а также метод
\texttt{to\_dict}, экспортирует конфигурацию модели в виде словаря для дальнейшей интеграции
с другими компонентами DEGANN. В функции \texttt{call} реализован прямой проход через нейросеть,
а вычисление градиентов производится стандартными средствами TensorFlow.
Разработанная оболочка обеспечивает унифицированный интерфейс для работы модели в системе.


Основным элементом данной реализации является класс \texttt{TensorflowGRUNet},
который наследуется от \texttt{tf.keras.Model}. Он отвечает за:
\begin{itemize}
  \item Построение и инициализацию GRU-слоёв, каждый из которых имеет число нейронов (определяемое параметром \textbf{block\_size}),
  \item Настройку ключевых параметров:
    \begin{itemize}
      \item \textbf{input\_size} --- размер входных данных (количество признаков);
      \item \textbf{output\_size} --- размер выхода;
      \item \textbf{activation\_func} и \textbf{recurrent\_activation} --- функции активации для основного и рекуррентного состояний (по умолчанию \textit{tanh} и \textit{sigmoid});
      \item \textbf{weight} и \textbf{biases} --- инициализаторы весов и смещений.
    \end{itemize}
  \item Организацию прямого прохода данных через сеть с помощью метода \texttt{call}.
  В этом методе входные данные последовательно обрабатываются всеми GRU-слоями, а затем результат передается в выходной полносвязный слой с линейной активацией, что оптимально для регрессионных задач.
\end{itemize}


\podrazdel{Внедрение}


Интеграция архитектуры GRU в DEGANN осуществляется посредством параметра \texttt{net\_type},
передаваемого при создании модели через базовый класс \texttt{IModel}.
При установке \texttt{net\_type="GRUNet"} внутри \texttt{IModel} вызывается метод,
создающий экземпляр класса \texttt{TensorflowGRUNet}.
Такой подход позволил сохранить единообразный интерфейс работы с моделями различных типов (например, с DenseNet или другими),
не нарушая общую модульную структуру проекта.

Особое внимание уделено совместимости новой реализации с существующей инфраструктурой DEGANN.
Это обеспечивает возможность использования стандартных методов обучения, тестирования и экспорта моделей.
Например, метод \texttt{ToDict} позволяет сохранить ключевые параметры модели (тип сети, количество слоёв,
число нейронов в каждом слое, размер выхода) в виде словаря для последующей интеграции с другими компонентами системы.


\razdel{Валидация}


\podrazdel{Параметры эксперемента}


В данном разделе представлено экспериментальное сравнение эффективности двух архитектур, реализованных в DEGANN:
GRUNet (на основе рекуррентных слоёв GRU) и DenseNet (традиционный многослойный перцептрон, MLP).
Цель эксперимента — продемонстрировать, что при одинаковых гиперпараметрах (5 слоёв, 30 нейронов
в каждом, функция потерь MSE, метрики MAPE, $R^2$) и фиксированном времени обучения архитектура
GRU обеспечивает более точное приближение целевых функций за счёт своей способности учитывать последовательные
зависимости в данных.

\textbf{Конфигурация тестирующей системы.}
\begin{itemize}
    \item Процессор: Intel i5 1240p
    \item Видео-чип: Iris Xe Graphics G7 80EUs
    \item ОЗУ: 16 гб
\end{itemize}

\textbf{Тестовые функции.} Для проверки моделей используются следующие функции, встроенные в DEGANN:
\begin{itemize}
    \item $\displaystyle f_{\sin}(x)=\sin(10x)$,
    \item $\displaystyle f_{\text{hyperbol}}(x)=\frac{x^2+0.5}{x+0.1}$,
    \item $\displaystyle f_{\text{hardsin}}(x)=\sin\Bigl(\ln\bigl(x^{\sin(10x)}\bigr)\Bigr)$.
\end{itemize}


\textbf{Обоснование выбора тестовых функций.} Выбранные функции являются встроенными в DEGANN
и представляют собой достаточно сложные для аппроксимации зависимости.
Функция $\displaystyle f_{\sin}(x)=\sin(10x)$ демонстрирует периодическую нелинейность,
что позволяет проверить способность модели захватывать колебательные процессы.
Функция $\displaystyle f_{\text{hyperbol}}(x)=\frac{x^2+0.5}{x+0.1}$ обладает неоднородной
зависимостью, где квадратичный рост сочетается с особенностями в малых значениях $x$, что требует
от модели адаптивного поведения в различных областях определения.
Функция $\displaystyle f_{\text{hardsin}}(x)=\sin\Bigl(\ln\bigl(x^{\sin(10x)}\bigr)\Bigr)$,
включающая логарифмическое преобразование и экспоненциальные эффекты, ещё более усложняет задачу
аппроксимации за счёт усиленной нелинейности.
Такой набор тестовых функций позволяет всесторонне оценить эффективность сравниваемых
архитектур, поскольку каждая из них предъявляет уникальные требования к способности
модели моделировать сложные, неоднородные зависимости.

\textbf{Параметры:} Для каждой архитектуры эксперименты проводятся при фиксированном времени обучения.
В частности, результаты тестирования собираются для трёх вариантов времени обучения: 15, 30 и 45 секунд.
При каждом варианте фиксируются следующие показатели:
\begin{itemize}
    \item \textbf{Функция потерь:} Mean Squared Error (MSE),
    \item \textbf{Метрики:} Mean Absolute Percentage Error (MAPE) и коэффициент детерминации ($R^2$)
    \item Дополнительно, фиксируются затраты оперативной памяти и общее время обучения.
\end{itemize}

Таким образом, основная задача эксперимента состоит в сравнении показателей качества
аппроксимации для GRUNet и DenseNet при идентичных условиях обучения.
При этом важно отметить, что использование одинаковых гиперпараметров позволяет провести объективное сравнение,
где разница в результатах обусловлена внутренней структурой моделей, а не различиями в настройках.


\podrazdel{Резултаты эксперемента}


Ниже приведены три таблицы, демонстрирующие структуру представления результатов эксперимента для каждого
из выбранных интервалов времени обучения.


\Table{Результаты эксперимента (время обучения: 15 секунд)}{|p{2cm}|p{1.5cm}|p{0.7cm}|p{1cm}|p{0.7cm}|p{1cm}|p{0.7cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Эпох} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet & hyperbol & 155 & 112.460 & 0.002 & 1.536 & 0.993 \nextline
GRUNet & sin      & 155 & 111.360 & 0.003 & 4.149 & 0.994 \nextline
GRUNet & hardsin  & 155 & 112.450 & 0.045 & 14.647 & 0.738 \nextline
DenseNet & sin    & 215 & 35.000  & 0.091 & 23.095 & 0.812 \nextline
DenseNet & hardsin& 215 & 41.770  & 0.017 & 10.100 & 0.902 \nextline
DenseNet & hyperbol & 215 & 33.990 & 0.051 & 13.346 & 0.819 \nextline
}


\Table{Результаты эксперимента (время обучения: 30 секунд)}{|p{2cm}|p{1.5cm}|p{0.7cm}|p{1cm}|p{0.7cm}|p{1cm}|p{0.7cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Эпох} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet   & hardsin  & 330 & 112.62 & 0.004 & 4.008 & 0.976 \nextline
GRUNet   & hyperbol & 330 & 116.27 & 0.005 & 3.508 & 0.980 \nextline
DenseNet & sin      & 445 & 37.52  & 0.012 & 7.509 & 0.975 \nextline
DenseNet & hyperbol & 445 & 36.95  & 0.011 & 5.989 & 0.961 \nextline
DenseNet & hardsin  & 445 & 44.78  & 0.005 & 5.101 & 0.971 \nextline
GRUNet   & sin      & 345 & 115.39 & 0.003 & 4.169 & 0.994 \nextline
}


\Table{Результаты эксперимента (время обучения: 45 секунд)}{|p{2cm}|p{1.5cm}|p{0.7cm}|p{1cm}|p{0.7cm}|p{1cm}|p{0.7cm}|}{
\textbf{Архитектура} & \textbf{Тестовая функция} & \textbf{Эпох} & \textbf{Память (MB)} & \textbf{Loss MSE} & \textbf{MAPE} & \textbf{$R^2$} \nextline
GRUNet   & sin      & 450 & 120.59 & 0.003 & 4.101 & 0.995 \nextline
GRUNet   & hardsin  & 470 & 116.84 & 0.002 & 3.158 & 0.987 \nextline
GRUNet   & hyperbol & 450 & 117.79 & 0.002 & 1.174 & 0.992 \nextline
DenseNet & sin      & 530 & 38.98  & 0.007 & 7.339 & 0.985 \nextline
DenseNet & hyperbol & 600 & 47.98  & 0.004 & 3.138 & 0.987 \nextline
DenseNet & hardsin  & 600 & 47.71  & 0.006 & 5.741 & 0.962 \nextline
}


\podrazdel{Вывод}

Анализ проведённых экспериментов демонстрирует, что архитектура GRUNet при идентичных гиперпараметрах
и фиксированном времени обучения обеспечивает более высокое качество аппроксимации целевых функций
по сравнению с DenseNet. По метрикам MAPE и коэффициенту детерминации ($R^2$) наблюдается стабильное преимущество GRUNet,
что указывает на его лучшую способность учитывать последовательные зависимости в данных.

Несмотря на несколько более высокие затраты памяти, разница в потреблении ресурсов
(порядка 80 МБ) не является значительной для пользователей в большинстве практических конфигураций,
в отличие от времени. GRUNet демонстрирует более быструю сходимость, обеспечивая лучшее качество
аппроксимации за меньшее количество эпох. Это делает данную архитектуру перспективной для задач
аппроксимации дифференциальных уравнений в рамках пакета DEGANN, где важна эффективность обучения при
сохранении высокой точности результатов.


\begin{thebibliography}{99}
\bibitem{degann} Пакет нейросетевой аппроксимации дифференциальных уравнений // DEGANN URL: https://github.com/Krekep/degann (дата обращения: 10.03.2025).
\bibitem{Deep Learning} Goodfellow I., Bengio Y., Courville A. Deep Learning. 1-е изд. Кембридж,Массачусетс, США: MIT Press, 2016. 800 с.
\bibitem{IBM_RNN} SStryker C. What is a Recurrent Neural Network? // IBM . 2024.
\bibitem{tensorflowDoc} Документация к пользованию tensorflow // TFnet URL: https://tensorflownet.readthedocs.io (дата обращения: 04.03.2025).
\bibitem{HOML} Geron A. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 3-е изд. Себастопол, Калифорния, США: O’Reilly Media, Inc., 8.11.2022. 856  с.
\end{thebibliography}



\end{document}

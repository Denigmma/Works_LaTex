% !TeX spellcheck = ru_RU
% !TEX root = vkr.tex

\section*{Введение}
\thispagestyle{withCompileDate}

Дифференциальные уравнения являются важным инструментом для описания процессов во многих областях науки и техники, включая физику, химию, биологию и экономику. Однако с усложнением и увеличением размерности задачи традиционные численные методы решения дифференциальных уравнений начинают сталкиваться с проблемами, связанными с длительным временем вычислений и недостаточной точностью. Это стимулирует поиск новых подходов к решению таких задач, одним из которых является пакет нейросетевой аппроксимации дифференциальных уравнений DEGANN\cite{degann}.

Однако на текущий момент в пакете реализована только одна архитектура нейронной сети — полносвязная сеть, что ограничивает возможности системы в аппроксимации сложных уравнений. Это создаёт необходимость в расширении пакета за счет добавления новых архитектур нейронных сетей, которые позволят улучшить точность и эффективность работы системы.

Рекуррентные нейронные сети (\textit{Recurrent Neural Networks}, RNN\cite{IBM_RNN}) представляют собой перспективный подход для задач аппроксимации дифференциальных уравнений, поскольку, в отличие от полносвязных сетей, они используют механизм запоминания состояний предыдущих слоёв. Это позволяет учитывать накопленную информацию из предыдущих шагов, что делает их более эффективными для моделирования сложных зависимостей и улучшает согласованность выходных значений. Однако RNN имеют известную проблему затухания градиента, из-за чего обучение становится затруднительным на длинных последовательностях.

Архитектура GRU (\textit{Gated Recurrent Unit} --- «Рекуррентный блок с управляемыми элементами»\cite{gru}) является усовершенствованной версией RNN и частично решает эту проблему благодаря использованию специальных элементов управления потоком данных — “элемента обновления” (update gate) и “элемента сброса” (reset gate). Эти элементы регулируют, какую часть информации из предыдущих состояний следует сохранить, а какую — игнорировать, что позволяет сети избегать накопления нерелевантной информации. За счёт этого GRU демонстрирует высокую стабильность в обучении, сохраняя способность учитывать долгосрочные зависимости. Такое сочетание компактной структуры (меньшее количество параметров по сравнению с LSTM\cite{lstm}) и способности эффективно моделировать зависимости делает GRU особенно подходящей для задач, связанных с аппроксимацией решений сложных дифференциальных уравнений.

Целью данной работы является реализация архитектуры GRU в пакете DEGANN\cite{degann} и проведение экспериментов, демонстрирующих её в задачах аппроксимации решений дифференциальных уравнений. Это позволит не только расширить функционал пакета, но и повысить его практическую применимость для сложных задач в научных и инженерных приложениях.
